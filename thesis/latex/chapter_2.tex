In this chapter, I present a solver and prove that it works, at least qualitatively, by using typical networks.

ALL THEORY

%#############################################################################
\section{Solver}

%=============================================================================
\subsection{Simplifications}

Plane wave.
Single mode.

Not only a list, I must justify them.
Anything below -40 dB can pretty much be ignored \todo{why?}.
What does not couple is lost, so just model the higher modes with a loss parameter.
This is going to work unless higher modes can come back to the fundamental mode, but that should be a very small effect.



%=============================================================================
\subsection{Jones calculus in 3D}

%=============================================================================
\subsection{Scattering matrices of Jones matrices}

%=============================================================================
\subsection{Solving a system}

%-----------------------------------------------------------------------------
\paragraph{Theory.}

We call $S$ the scattering matrix of the entire system.
\begin{equation*}
    S =
    \begin{pmatrix}
        S_{1, 1} & S_{1, 2} & \cdots & S_{1, n} \\
        S_{2, 1} & S_{2, 2} & \cdots & S_{2, n} \\
        \vdots   & \vdots   & \ddots & \vdots \\
        S_{n, 1} & S_{n, 2} & \cdots & S_{n, n}
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    b = Sa
\end{equation*}
S contains information about all the ports in the system: the ports that are open to the outside world, but also the ports hidden within the system because the networks are coupled to each other.
Here, $n$ is the number of ports of $S$, equal to the sum of the number of ports of all the networks constituting the system.

Some of the networks constituting the system are coupled by one port.
If two networks G and H are coupled by their port $g$ and $h$, then the output of $g$ is the input of $h$ and the output of $h$ is the input of $g$, as illustrated by Equation~\eqref{eq:coupled_inputs_outputs}.
\begin{equation}
    \left\lbrace
    \begin{aligned}
        b_h &= a_g \\
        b_g &= a_h
    \end{aligned}
    \right.
    \label{eq:coupled_inputs_outputs}
\end{equation}

Ports that are coupled are called ``inside port'', those that are not coupled are called ``outside port''.
If we note~$a^o$ the inputs from the outside world,~$b^o$ the outputs to the outside world,~$a^i$ the inputs inside the system and~$b^i$ the outputs outside the system, then we can reorder the rows and columns of the vectors~$a$,~$b$ and the matrix~$S$ to put together the inside ports and the outside ports: 
\begin{equation}
    \binom{b^o}{b^i} =
    \begin{pmatrix}
        S^{oo} & S^{oi} \\
        S^{io} & S^{ii} \\
    \end{pmatrix}
    \binom{a^o}{a^i}
    \label{eq:s_outside_inside}
\end{equation}
The four regions of~$S$ have the following physical meaning:
\begin{description}
    \item[$S^{oo}$:] from outside to outside: signal that is reflected on the entrance ports and therefore never enters the system.
    \item[$S^{io}$:] from outside to inside: signal that enters the system.
    \item[$S^{ii}$:] from inside to inside: transmissions and reflections internal to the system.
    \item[$S^{oi}$:] from inside to outside: signal that leaves the system.
\end{description}
Only~$a^o$ is known: this corresponds to the input to the system and we control it.
$a^i$,~$b^i$ and~$b^o$ are unknown.
$b^o$ is the result we are looking for.
$b^i$ and~$a^i$ are nice to have as they allow us to see what is happening inside the system.

\subparagraph{Solving the internal transmissions and reflections.}
How are~$a^i$ and~$b^i$ related?
Equation~\eqref{eq:coupled_inputs_outputs} indicates that each element of $a^i$ is equal to an element of $b^i$, therefore $a^i$ can be obtain by reordering $b^i$ and vice-versa.
\index{permutation matrix}In other words, there exists a permutation matrix~$P$ such that
\begin{equation}
    a^i = P b^i \text{.} \label{eq:relation_ai_bi}
\end{equation}

To get $b_o$, the first step is to compute~$b_i$ from~$a_o$.
\begin{subequations}
    \begin{align}
        b^i &= S^{io}a^o + S^{ii}a^i \label{eq:compute_bi_ai} \\
        b^i &= S^{io}a^o + S^{ii}Pb^i \label{eq:compute_bi_bi} \\
        (I - S^{ii}P)b^i &= S^{io}a^o \label{eq:compute_bi_solve} \\
        b^i &= (I - S^{ii}P)^{-1} S^{io}a^o \label{eq:compute_bi_invert}
    \end{align} \label{eq:compute_bi}
\end{subequations}
where~$I$ is the identity matrix with a dimension equal to that of~$S^{ii}P$.

Can we go from~\eqref{eq:compute_bi_solve} to~\eqref{eq:compute_bi_invert}?
The matrix~$I - S^{ii}P$ has an inverse if and only if it is not singular.
A matrix is singular if and only if its determinant equals~0.
Can the determinant of~$I - S^{ii}P$ equal~0 (equation~\eqref{eq:det_i_minus_siip})?
\begin{equation}
    \det(I - S^{ii}P) = 0 \label{eq:det_i_minus_siip}
\end{equation}
\index{eigenvalue}Equation~\eqref{eq:det_i_minus_siip} has the shape of an eigenvalue problem;
equation~\eqref{eq:eigenvalue_typical} defines the eigenvalues~$\lambda$ of a matrix~$A$.
\begin{equation}
    \det(A - \lambda I) = 0 \label{eq:eigenvalue_typical}
\end{equation}
In our case, $\lambda=1$ and $A=S^{ii}P$.
However, the sign is not the same: $I-A \neq A-I$.
Fortunately, the determinant has the following property~\eqref{eq:determinant_scalar_multiplication}:
\begin{equation}
    \det(c A) = c^n \det(A) \label{eq:determinant_scalar_multiplication}
\end{equation}
for any $n$--by--$n$ matrix $A$.
Therefore,
\begin{subequations}
    \begin{align}
        \det(I - S^{ii}P)
        &= \det(-1(S^{ii}P - I)) \\
        &= (-1)^n \det(S^{ii}P - I) \text{.}
    \end{align}
\end{subequations}
We do not need to worry about the parity of~$n$.
Indeed, if $x=0$ then $(-1)^n x = 0$ as well.
This means that for our eigenvalue problem, the sign does not matter, as summarized by equation~\eqref{eq:determinant_sign_does_not_matter}.
\begin{equation}
    \det(I - S^{ii}P) = 0
    \quad
    \Longleftrightarrow
    \quad
    \det(S^{ii}P - I) = 0
    \label{eq:determinant_sign_does_not_matter}
\end{equation}
\index{eigenvector}We have succesfully identified our question of the invertibility of $I - S^{ii}P$ with a question regarding the eigenvectors of $S^{ii}P$ for the eivenvalue 1.
Saying $\det(S^{ii}P - I) = 0$ is equivalent to saying that there exist non-zero vectors $v$, called ``eigenvectors'', such that $v = S^{ii}Pv$.
The following argument, based on physical considerations, will demonstrate that there are no such vectors.

$b^i$ is what $S^{ii}P$ is applied to in equation~\eqref{eq:compute_bi}, therefore we are looking for vectors $b^i$ satisfying
\begin{equation}
    b^i = S^{ii}P b^i \text{.} \label{eq:bi_eigenvector}
\end{equation}
The two equations~\eqref{eq:bi_eigenvector} and ~\eqref{eq:compute_bi_bi}, taken together, 
lead to the following interesting equality:
\begin{equation}
    \left\lbrace
        \begin{aligned}
            b^i &= S^{ii}P b^i \\
            b^i &= S^{io}a^0 + S^{ii}P b^i
        \end{aligned}
    \right.
    \quad
    \Longleftrightarrow
    \quad
    S^{io}a^o = 0
\end{equation}
The only way for $S^{io}a^o$ to equal 0 for any value of $a^o$ is for $S^{io}$ to contain only zeros.
In other words, the system is totally blind: no energy from the outside world ($a^o$) ever enters the system.
Blind systems cannot be solved by our method but this is not a problem since there is no point in trying to build them in the first place.
For the rest of this demonstration, let us assume that the system is not blind, $S^{io}\neq 0$, but that instead we merely set $a^o$ to 0; let us also assume $b^i \neq 0$ (we somehow managed to inject energy inside the system before it got blind).
What would $b^i = S^{ii}P b^i$ mean?

$b^i$ is the list of all the outputs of the inside part of the system, fed back as input to that same system.
What equation~\eqref{eq:bi_eigenvector} means is that there exists some fields that are unaffected by the system; if such a field would enter the system, it would be trapped inside the system, looping through $S^{ii}P$ forever.
If $S^{ii}P$ has an eigenvector, then our system is totally lossless (for some fields) and can store any amount of energy forever.
We know that all our networks have losses; even the propagation in free space has losses in the form of the beam diffracting slowly and becoming wider than its target, essentially leaking energy.
Without using active components (external energy source, amplifiers), the best we can do is to aim for very low losses, which would produce very high-Q cavities but not perpetual storages.
Because our systems have losses, $S^{ii}P$ has no eigenvectors.
Therefore $S^{ii}P$ has no eigenvalues.
Therefore 1 is not an eigenvalue.
Therefore $\det(I-S^{ii}P) \neq 0$.
Therefore $I-S^{ii}P$ has an inverse.
Therefore equation~\eqref{eq:compute_bi_invert} is correct, and the system can be solved.

It can be interesting to draw a parallel between our problem and the convergence of an infinite series.
Indeed, an infinite series of ratio $q$ converges if $\vert q \vert < 1$:
\begin{equation}
    \sum_{i=0}^\infty q^i = \frac{1}{1-q} = (1-q)^{-1}
\end{equation}
The ressemblance with $(I - S^{ii}P)^{-1}$ is not due to chance.
Our system acts like a cavity, maybe a very complex cavity but a cavity nontheless.
Therefore we can think about it as a simple cavity delimited by two semi-transparent mirrors.
The signal trapped inside the cavity is reflected an infinite amount of times, each double-reflection attenuates the signal by a factor $q=S^{ii}P$.
As a result, the signal inside the cavity is the signal that enters the cavity multiplied by $1+q+q^2+q^3+\dots$ up to infinity.
If there are losses, then $|q|<1$ and that series converges to $(1-q)^{-1}$ or, considering we are working with matrices, $I - S^{ii}P$.

Note that for the implementation, we may want to avoid inverting the matrix.
Indeed, inverting matrices is costly and numerically unstable \todo{citation needed}.
Even though equation~\eqref{eq:compute_bi_invert} is mathematically correct, we would rather solve $b^i$ in equation~\eqref{eq:compute_bi_solve}.

\subparagraph{Solving the output of the system.}
Once we know the internal reflections of the system ($b^i$ as a function of itself), getting its output is trival.
The second step is to compute~$b^o$ from~$a^o$ and~$b^i$.
We do that by taking $b^o$ from equation~\eqref{eq:s_outside_inside} and eliminating $a^i$ with equation~\eqref{eq:relation_ai_bi}.
\begin{align}
    b^o &= S^{oo}a^o + S^{oi}a^i \notag \\
    b^o &= S^{oo}a^o + S^{oi}Pb^i \label{eq:compute_bo}
\end{align}

These two steps solve the system: from the inputs~$a^o$ we can compute the outputs~$b^o$ of the whole system.
This method also tells gives us $a^i$ and $b^i$ which are two equivalent ways of looking at what is happening inside the system.

%-----------------------------------------------------------------------------
\paragraph{Implementation.}
Although the mathematics are quite simple, the implementation requires careful book keeping of the many indices of the many matrices.

\begin{itemize}
    \item 
The system contains $N$ networks.
    \item 
Each network is modeled with a~$n_i$--by--$n_i$ scattering matrix, where~$n_i$ is the number of ports the network $i$, with $i \in \llbracket 1, N \rrbracket$.
    \item 
The whole system is modeled with a~$n$--by--$n$ scattering matrix, where~$n$ is the number of ports of the whole system, with $n = \sum_{i=1}^N n_i$.
\end{itemize}

Each network $i$ comes with ports that are ordered from 1 to $n_i$.
The networks themselves are ordered since they are indexed from 1 to $N$.
Therefore, there is a natural way of numbering the ports of the whole system.
The port~$j$ of the network~$i$ corresponds to the port~$k$ of the whole system, where
\begin{equation}
    k = \sum_{m=0}^{i - 1}n_m + j \label{eq:port_numbering}
\end{equation}

With that information, it is trivial to fill the scattering matrix of the whole system, $S$, with values from the scattering matrices from each network $S_i$.
$S$ contains mostly zeros, except for blocks on its diagonal containing copies of each~$S_i$.
For example, Equation~\eqref{eq:block_scattering} shows what $S$ looks like for a system containing two networks $S_1$ and $S_2$.
\begin{equation}
    S =
    \begin{pmatrix}
        {S_1}_{1, 1} & \cdots & {S_1}_{1, n_1} &
        0 & \cdots & 0
        \\
        \vdots & \ddots &\vdots &
        \vdots & \ddots &\vdots 
        \\
        {S_1}_{n_1, 1} & \cdots & {S_1}_{n_1, n_1} &
        0 & \cdots & 0
        \\
        0 & \cdots & 0 &
        {S_2}_{1, 1} & \cdots & {S_2}_{1, n_2}
        \\
        \vdots & \ddots &\vdots &
        \vdots & \ddots &\vdots
        \\
        0 & \cdots & 0 &
        {S_2}_{n_2, 1} & \cdots & {S_2}_{n_2, n_2}
    \end{pmatrix}
    \label{eq:block_scattering}
\end{equation}
Note that the number of zeros increases with~$N^2$ while the number of non-zeros elements around the diagonal increases with~$N$.
This remark has no importance from a purely mathematical point of view but is very important for the implementation of the solving algorithm.
\index{sparse matrix}Indeed, matrices that contain many zeros are called ``sparse matrices'' and many algorithms exist that have been optimized for dealing with them.
If we use such an optimized algorithm, then the matrix inversion that appears in Equation~\eqref{eq:compute_bi_invert} will execute in $O(N)$ instead of $O(N^2)$ (using the ``big O notation'').

Unless we are lucky, the system scattering matrix $S$ (that we build from the network scattering matrices $S_i$) is not organized into inside and outside sectors like shown on Equation~\eqref{eq:s_outside_inside}.
\index{permutation matrix}However, there exists a permutation matrix~$Q$ such as
\begin{gather}
    \left\lbrace
    \begin{aligned}
        S' &= Q S Q^{-1} \\
        a' &= Q a \\
        b' &= Q b
    \end{aligned}
    \right.
    \label{eq:permute_s}
    \\
    b = S a \quad \Longleftrightarrow \quad b' = S' a' \label{eq:permute_s_equiv}
\end{gather}
with $S'$ having the form of Equation~\eqref{eq:s_outside_inside}.

The permutation matrix $Q$ must not be confused with the permutation matrix $P$ from equation~\eqref{eq:relation_ai_bi}, they are distinct, and both are needed.
$Q$~separates the inside ports from the outside ports, while $P$~describes how the inside ports are coupled.
Once the permutation matrix~$Q$ is known, we can solve the problem for~$S'$ and get~$b'$, from which we can retrieve~$b$ (Equation \eqref{eq:permute_s_equiv}).

Permutation matrices permute the order of rows and columns of matrices.
They contain one and only one 1 per row and per column, all the other elements are 0.
Pre-multiplying by a permutation matrix changes the order of the rows.
Post-multiplying by a permutation matrix changes the order of the columns.
The transpose of a permutation matrix is also a permutation matrix.
The transpose of a permutation matrix is also its inverse: $P\transp = P^{-1}$.
This last property is important for optimization purposes because inverting matrices is expensive and unstable.

Both permutation matrices~$P$ and~$Q$ can be derived from a single dataset.
That dataset is a description of the ports that are coupled.
That dataset obviously defines~$P$, the matrix that describe the couplings.
But it also defines~$Q$, the matrix that separates the inside ports from the outside ports.
Indeed, any port present in that dataset is by definition an inside port, and any port absent from that dataset is an outside port.

\index{coupling descriptor}One way of representing the coupling of two ports is with a set of cardinal~2 that I name ``coupling descriptor''.
Its two elements are the two identifiers of the ports that are coupled.
For example, if the port 4 of the system is coupled to the port 10, then we can represent that coupling with the set $\lbrace 4, 10\rbrace$ which is equal to the set $\lbrace 10, 4\rbrace$ (sets are unordered).
Enforcing a cardinal of 2 prevents the coupling of a port to itself: $\lbrace 4, 4\rbrace = \lbrace 4\rbrace$ which has a cardinal of~1.
To be valid, each element must be between 1 and $n$, the number of ports in the whole system.

A set of coupling descriptors defines all the couplings that exist in the system.
To be valid, each of the coupling descriptors that it contains must be valid, and each port identifier can appear at most once.
For example,
$\lbrace \lbrace 1, 2 \rbrace, \lbrace 5, 3 \rbrace \rbrace$
is valid, but 
$\lbrace \lbrace 1, 2 \rbrace, \lbrace 5, 2 \rbrace \rbrace$
is not because the port identifier~2 appears more than once.

There are many possible permutation matrices $Q$: all that is required for $Q$ is to separate the inside ports from the outside ports, there is no constraint on the order of the ports beyond that.
To ensure the reproducibility of our algorithm, we want to choose one specific $Q$, the one for which the inside and outside port identifiers are in ascending order.
This is what the algorithm~\ref{algo:separate_inside_outside} provides.
Algorithm~\ref{algo:separate_inside_outside} uses the outputs of algorithms~\ref{algo:find_inside} and~\ref{algo:find_outside}.

\begin{algorithm}
    \caption{FindInside}
    \label{algo:find_inside}
    \begin{algorithmic}
        \REQUIRE {\textit{couplings}} \COMMENT{a valid set of coupling descriptors}
        \STATE {\textit{inside} $\leftarrow \emptyset$}
        \FORALL{\textit{coupling} $\in$ \textit{couplings}}
            \STATE {\textit{inside} $\leftarrow$ \textit{inside} $\cup$ \textit{coupling}}
        \ENDFOR
        \RETURN {\textit{inside}} \COMMENT{a set containing all the coupled ports}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{FindOutside}
    \label{algo:find_outside}
    \begin{algorithmic}
        \REQUIRE {\textit{inside}} \COMMENT{a set containing all the coupled ports}
        \REQUIRE {$n$} \COMMENT{number of ports in the system}
        \STATE{\textit{allports} $\leftarrow \lbrace 1, 2, 3, \dots, n \rbrace$}
        \STATE{\textit{outside} $\leftarrow$ \textit{allports} $\setminus$ \textit{inside}}
        \RETURN {\textit{outside}} \COMMENT{a set containing all the non-coupled ports}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{SeparateInsideOutside}
    \label{algo:separate_inside_outside}
    \begin{algorithmic}
        \REQUIRE {\textit{inside}} \COMMENT{a set containing all the coupled ports}
        \REQUIRE {\textit{outside}} \COMMENT{a set containing all the non-coupled ports}
        \REQUIRE {$n$} \COMMENT{number of ports in the system}
        \STATE{\textit{insorted} $\leftarrow$ sort(array(\textit{inside}))}
        \STATE{\textit{outsorted} $\leftarrow$ sort(array(\textit{outside}))}
        \STATE {$Q \leftarrow$ $n$--by--$n$ matrix filled with 0}
        \FOR {$i = 1$ to length(\textit{outsorted})}
            \STATE{$Q_{i, \textit{outsorted}[i]} \leftarrow$ 1}
        \ENDFOR
        \FOR {$i$ = 1 to length(\textit{insorted})}
            \STATE{$Q$[$i$ + length(\textit{outsorted}), \textit{insorted}$[i]$] $\leftarrow$ 1}
        \ENDFOR
        \RETURN {$Q$} \COMMENT{Permutation matrix that separates inside/outside ports}
    \end{algorithmic}
\end{algorithm}

Example: A system with $n=4$ ports has its ports 1 and 3 coupled together.
Algorithm~\ref{algo:separate_inside_outside_example} shows how to compute $Q$ from a set of coupling descriptors.
The ``Ensure'' statements are assertions, propositions that must be true if the algorithm is working properly.

\begin{algorithm}[H]
    \caption{SeparateInsideOutside, example}
    \label{algo:separate_inside_outside_example}
    \begin{algorithmic}
        \STATE {couplings $\leftarrow \lbrace \lbrace 3, 1 \rbrace \rbrace$}
        \STATE {$n \leftarrow 4$}
        \STATE {inside $\leftarrow$ FindInside(couplings)}
        \ENSURE {inside = $\lbrace 3, 1 \rbrace$} \COMMENT {Order does not matter for set equality.}
        \STATE {outside $\leftarrow $ FindOutside(inside, $n$)}
        \ENSURE {outside $= \lbrace 2, 4 \rbrace$}
        \STATE {$Q \leftarrow$ SeparateInsideOutside(inside, outside, $n$)}
        \ENSURE {
        $
        Q = \begin{pmatrix}
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1\\
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0
        \end{pmatrix}
        $
        }
    \end{algorithmic}
\end{algorithm}
Let us apply the permutation matrix $Q$ produced in the example algorithm~\ref{algo:separate_inside_outside_example} to a vector~$a$ and a scattering matrix~$S$, following equation~\eqref{eq:permute_s}.
The results are given in equations~\eqref{eq:q_a_aprime} and~\eqref{eq:q_s_qtranspose_sprime}.
\begin{equation}
    Q a
    =
    \begin{pmatrix}
        0 & 1 & 0 & 0\\
        0 & 0 & 0 & 1\\
        1 & 0 & 0 & 0\\
        0 & 0 & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
        a_1 \\ a_2 \\ a_3 \\ a_4
    \end{pmatrix}
    =
    \begin{pmatrix}
        a_2 \\ a_4 \\ a_1 \\ a_3
    \end{pmatrix}
    =
    a'\text{.}
    \label{eq:q_a_aprime}
\end{equation}
\begin{equation}
    Q S Q^{-1}
    =
    \begin{pmatrix}
        S_{2,2} & S_{2,4} & S_{2,1} & S_{2,3} \\
        S_{4,2} & S_{4,4} & S_{4,1} & S_{4,3} \\
        S_{1,2} & S_{1,4} & S_{1,1} & S_{1,3} \\
        S_{3,2} & S_{3,4} & S_{3,1} & S_{3,3}
    \end{pmatrix}
    =
    S'
    \label{eq:q_s_qtranspose_sprime}
\end{equation}
The next step, separating $a'$ in two and $S'$ in four, is trivial to implement.
Equation~\eqref{eq:q_s_qtranspose_sprime_decomposed} shows a decomposition of the matrix $S'$ from equation~\eqref{eq:q_s_qtranspose_sprime} into inside and outside submatrices.
\begin{equation}
    \begin{aligned}
    S'^{oo}
    &=
    \begin{pmatrix}
        S_{2,2} & S_{2,4}  \\
        S_{4,2} & S_{4,4}  \\
    \end{pmatrix}
    &
    S'^{oi}
    &=
    \begin{pmatrix}
        S_{2,1} & S_{2,3}  \\
        S_{4,1} & S_{4,3}  \\
    \end{pmatrix}
    \\
    S'^{io}
    &=
    \begin{pmatrix}
        S_{1,2} & S_{1,4}  \\
        S_{3,2} & S_{3,4}  \\
    \end{pmatrix}
    &
    S'^{ii}
    &=
    \begin{pmatrix}
        S_{1,1} & S_{1,3}  \\
        S_{3,1} & S_{3,3}  \\
    \end{pmatrix}
    \end{aligned}
    \label{eq:q_s_qtranspose_sprime_decomposed}
\end{equation}
Our algorithm successfully reorganized our data in a way compatible with equation~\eqref{eq:s_outside_inside}.

We now wish to construct the permutation matrix~$P$ that appears in equation~\eqref{eq:relation_ai_bi}.
Applied to our example, this corresponds to equation~\eqref{eq:relation_ai_bi_example}.
\begin{equation}
    \begin{pmatrix}
        a_1 \\ a_3
    \end{pmatrix}
    =
    P
    \begin{pmatrix}
        b_1 \\ b_3
    \end{pmatrix}
    \label{eq:relation_ai_bi_example}
\end{equation}
In our example, the ports 1 and 3 are coupled, therefore $a_1 = b_3$ and $a_3 = b_1$, which leads to equation~\eqref{eq:relation_bi_bi_example}.
\begin{equation}
    \begin{pmatrix}
        b_3 \\ b_1
    \end{pmatrix}
    =
    P
    \begin{pmatrix}
        b_1 \\ b_3
    \end{pmatrix}
    \label{eq:relation_bi_bi_example}
\end{equation}
It is obvious that the correct value for $P$ is that of equation~\eqref{eq:p_example}, but we need an algorithm to compute~$P$ for us in the general case.
\begin{equation}
    P =
    \begin{pmatrix}
        0 & 1 \\
        1 & 0
    \end{pmatrix}
    \label{eq:p_example}
\end{equation}
One difficulty comes from the fact that we cannot use the indices of the coupled ports directly to access the matrices $a'^i$, $b'^i$ and $S'^{ii}$.
Indeed, in our example, $b^i$ contains two elements only so we cannot reach $b_3$ by using directly the index 3.
We need to map the indices given by the set of coupling descriptors to indices that can be used to address the various matrices.
In our example, 1 is mapped to 1 ($b'^i_1 = b_1$), and 3 is mapped to 2 ($b'^i_2 = b_3$); therefore saying that ports 1 and 3 are coupled is equivalent to saying that indices 1 and 2 need to be permuted.
Algorithm~\ref{algo:port_to_index} converts a port identifier to an index usable to address $S'^{ii}$.
\begin{algorithm}
    \caption{PortToIndex}
    \label{algo:port_to_index}
    \begin{algorithmic}
        \REQUIRE{\textit{port}} \COMMENT{The port identifier for which we want the index.}
        \REQUIRE{\textit{inside}} \COMMENT{Set of all the inside port identifiers.}
        \STATE{\textit{insorted} $\leftarrow$ sort(array(\textit{inside}))}
        \STATE{\textit{index} $\leftarrow$ position of \textit{port} in \textit{insorted}}
        \RETURN{\textit{index}}
    \end{algorithmic}
\end{algorithm}

We can now construct the permutation matrix $P$ with algorithm~\ref{algo:couple_inputs_to_outputs}.
\begin{algorithm}
    \caption{CoupleInputsToOutputs}
    \label{algo:couple_inputs_to_outputs}
    \begin{algorithmic}
        \REQUIRE{\textit{couplings}}
        \STATE {\textit{inside} $\leftarrow$ FindInside(\textit{couplings})}
        \STATE {$m \leftarrow$ cardinal(\textit{inside})}
        \STATE {$P \leftarrow$ $m$--by--$m$ matrix filled zith 0}
        \FORALL {\textit{coupling} $\in$ \textit{couplings}}
            \STATE {\textit{couplingArray} $\leftarrow$ array(\textit{coupling})}
            \STATE {\textit{portA} $\leftarrow$ \textit{couplingArray}[1]}
            \STATE {\textit{portB} $\leftarrow$ \textit{couplingArray}[2]}
            \STATE {\textit{indexA} $\leftarrow$ PortToIndex(\textit{portA}, \textit{inside})}
            \STATE {\textit{indexB} $\leftarrow$ PortToIndex(\textit{portB}, \textit{inside})}
            \STATE {$P_{\textit{indexA}, \textit{indexB}} \leftarrow$ 1}
            \STATE {$P_{\textit{indexB}, \textit{indexA}} \leftarrow$ 1}
        \ENDFOR
        \RETURN {$P$}
    \end{algorithmic}
\end{algorithm}

If we apply algorithm~\ref{algo:couple_inputs_to_outputs} to our example, then there is only one value for \textit{coupling}: $\lbrace 1, 3\rbrace$.
Therefore, \textit{couplingArray}=[1, 3], \textit{portA}=1, \textit{portB}=3.
The two calls to PortToIndex take \textit{inside}=$\lbrace 1, 3\rbrace$ and make a sorted array from it, \textit{insorted}=[1, 3].
The position of \textit{portA} in that sorted array is 1, that of \textit{portB} is 2, so \textit{indexA}=1 and \textit{indexB}=2.
The matrix $P$ is filled with zeros, except for the elements $P_{1, 2}$ and $P_{2, 1}$ that equal 1, matching equation~\eqref{eq:p_example}.

We have now everything we need to solve equation~\eqref{eq:compute_bi_solve}, which I reproduce here (equation~\eqref{eq:compute_bi_solve_prime}) using the \textit{prime} version of the variables resulting from the permutations involving~$Q$.
\begin{equation}
    (I - S'^{ii})b'^i = S'^{io}a'o \label{eq:compute_bi_solve_prime}
\end{equation}
In equation~\eqref{eq:compute_bi_solve_prime}, everything is known except~$b'^i$.
We wish to solve this equation for~$b'^i$.
This is a very common problem for which many numerical packages offer solutions.
Solving systems of linear equations is also decribed at length in the chapter 2 of Numerical Recipes \cite{Press:2007:NRE:1403886}.
The developer is free to choose any method.
I suggest however something along the line of a LU decomposition.
Indeed, that LU decomposition can be computed once, its result stored, and then used several times to solve the same system for different values of $a'^o$.
Big systems may benefit from using sparse matrices.





%#############################################################################

\section{Generic networks}

Scattering matrix of typical networks, with parameters.

Some of these parameters may not be super physical, but are here to kinda absorb the imperfections and the mismatch when fitting.

For the grid I use Houde \cite{houde_2001} because there is nothing simpler, but for the other networks I keep it as simple as I can.
Grids being VERY complex to model, I might spend a lot of time and space on that one.

So, in the end, it's a list of scattering matrices.  It would be nice to augment each system with plots showing the effect of each parameter.



%=============================================================================

\subsection{Distance}
Let us assume an homogeneous isotropic propagation medium with an refractive index $n$.
$n$ can be complex.

\begin{figure}[hbtp]
    \centering
    \missingfigure{Propagation in a homogeneous medium.}
    \caption{\label{fig:net_distance} Propagation in a homogeneous medium.}
\end{figure}
Figure \ref{fig:net_distance} shows two points $z_1$ and $z_2$ along the path of a plane wave traveling through a medium of index $n$.
The line joining the two points is parallel to the direction of propagation of the wave.
The two points are separated by a distance $d$.
Then, at any time, the electric fields at the two points are linked by equation~\eqref{eq:net_distance}.
\begin{equation}
    e(z_2) = e(z_1) \exp
    \left(
        - i 2\pi d n f / c_0
    \right)
    \label{eq:net_distance}
\end{equation}

The argument of the exponential can be separated into a real and an imaginary part as shown with equation~\eqref{eq:absorption_phase}.
\begin{gather}
    \begin{aligned}
        - i 2\pi d n f / c_0
        &= - i 2\pi d \left(\Re(n) + i\Im(n)\right) f / c_0 \\
        &= 2\pi d f / c_0 \left(\Im(n) - i\Re(n) \right) \\
        &= a + i \phi
    \end{aligned}
    \label{eq:absorption_phase}
    \\
    \begin{aligned}
        a &= 2\pi d \Im(n) f / c_0   &   \phi &= -2\pi d \Re(n) f / c_0
    \end{aligned}
\end{gather}
The real part~$a$ constitutes an absorption factor while the imaginary part~$\phi$ constitutes a phase factor.
The imaginary part of the refractive index is small for dielectric and big for metals,
leading to a very strong absorption of the wave in metals.
The minus sign for the phase comes from the fact that the wave reaches $z_1$ before it reaches $z_2$: the field in $z_2$ is late relatively to the field in $z_1$.
Note that $\Re(n) f / c_0 = 1 / \lambda$, with $\lambda$ the wavelength of the wave in the medium.

A distance of homogeneous isotropic propagation medium constitutes a two-ports network (equation~\eqref{eq:s_2_ports}).
\begin{equation}
    S =
    \begin{pmatrix}
        S_{1, 1} & S_{1, 2} \\
        S_{2, 1} & S_{2, 2}
    \end{pmatrix}
    \label{eq:s_2_ports}
\end{equation}
There are no reflections on the ports (reflections occur at interfaces between materials of different refractive indices, we shall study these later).
\begin{equation}
    S_{1, 1} = S_{2, 2} =
    \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix}
\end{equation}
The transmission is the same both ways, and because of the isotropy, all three components of the field are affected in the same way.
\begin{equation}
    S_{1, 2} = S_{1, 2} =
    \exp(- i 2\pi d n f / c_0)
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
\end{equation}


%=============================================================================

\subsection{Interface at normal incidence}
\label{sec:interface_at_normal_incidence}

\begin{figure}[hbtp]
    \centering
    \missingfigure{Interface at normal incidence}
    \caption{\label{fig:net_interface_normal}Interface at normal incidence.}
\end{figure}
An interface is an implicit surface defined by a change in refractive index.
Generally, interfaces both reflect and transmit light.
The amount of reflection and transmission depends on the refractive indices on the two sides of the interface.
One particular case is the case where both indices are equal; in this case the interface does not really exist and has no effect on the wave: no reflection and full transmission.

When the direction of propagation is normal to the surface, we are in a case of normal incidence.
In case of normal incidence, the interface is a two-ports network and its scattering matrix has the shape of equation~\eqref{eq:s_2_ports}.

Equations~\eqref{eq:fresnel_normal} are the Fresnel equations~\eqref{eq:fresnel_oblique} rewritten for the case of normal incidence.
\begin{subequations}
    \begin{align}
        r &= \frac{n_i - n_t}{n_i + n_t} \label{eq:fresnel_r}\\
        t &= \frac{2 n_i}{n_i + n_t} \label{eq:fresnel_t}
    \end{align}
    \label{eq:fresnel_normal}
\end{subequations}
The $i$ and $t$ subscripts stand for ``incident'' and ``transmitted''.
The two materials, ``material 1'' and ``material 2'', have for refraction indices $n_1$ and $n_2$.
When when going from material 1 to material 2, $n_i = n_1$ and $n_t = n_2$.
When when going from material 2 to material 1, $n_i = n_2$ and $n_t = n_1$.
Therefore, the four elements of the scattering matrix are given by equation~\eqref{eq:s_interface_normal} in which $I_3$ is the 3--by--3 identity matrix.
\begin{subequations}
    \begin{align}
        S_{1, 1} &= \frac{n_1 - n_2}{n_1 + n_2} I_3
        \\
        S_{2, 2} &= \frac{n_2 - n_1}{n_2 + n_1} I_3
        \\
        S_{1, 2} &= \frac{2 n_2}{n_2 + n_1} I_3
        \\
        S_{2, 1} &= \frac{2 n_1}{n_1 + n_2} I_3
    \end{align}
    \label{eq:s_interface_normal}
\end{subequations}

%=============================================================================

\subsection{Interface at oblique incidence}
Interfaces at oblique incidence are more complex to model than interfaces at normal incidence because they need more parameters to describe.
In addition to the two refractive indices, we need the direction of propagation and the orientation of the interface in space.
This is required because the reflection and transmission on an oblique interface depend on the polarization of the wave seen by the surface.

\index{plane-of-incidence}As described by Hecht in \cite{hecht2002optics} (chapter 4), reflection and transmission depend on whether the field is contained in, or is normal to, the plane-of-incidence.
The plane-of-incidence is the plane that contains both the direction of propagation and the normal to the interface.
In case of normal incidence, the plane-of-incidence is undefined.
\begin{figure}[hbtp]
    \centering
    \missingfigure{Interface at oblique incidence}
    \caption{\label{fig:net_interface_oblique}Interface at oblique incidence.}
\end{figure}

\index{Fresnel equations}Equations~\eqref{eq:fresnel_oblique} are the Fresnel equations corresponding to the field directions described in figure \ref{fig:fresnel_directions} for linear homogeneous isotropic dielectric materials.
\begin{subequations}
    \begin{align}
        r_\parallel & =
        \frac{n_i \cos \theta_t - n_t \cos \theta_i}{n_i \cos \theta_t + n_t \cos \theta_i}
        \label{eq:fresnel_rp}
        \\
        r_\perp & =
        \frac{n_i \cos \theta_i - n_t \cos \theta_t}{n_i \cos \theta_i + n_t \cos \theta_t}
        \label{eq:fresnel_rs}
        \\
        t_\parallel & =
        \frac {2 n_i \cos \theta_i}{n_i \cos \theta_t + n_t \cos \theta_i}
        \label{eq:fresnel_tp}
        \\
        t_\perp & =
        \frac {2 n_i \cos \theta_i}{n_i \cos \theta_i + n_t \cos \theta_t}
        \label{eq:fresnel_ts}
    \end{align}
    \label{eq:fresnel_oblique}
\end{subequations}
Equation~\eqref{eq:fresnel_rp} differs from that given by Hecht in \cite{hecht2002optics}.
This comes from a different convention for the direction of the reflected field.
Our convention for the direction of the parallel reflected field makes the parallel and perpendicular results consistent when the angle of incidence is zero.

% In Hecht's chapter 4, when you try to get the equations to line up for the
% case of normal incidence, then the two transmissions have the same limit,
% but the two reflections disagree.  Why?  It should not be the case, should
% it?  Which one is the correct one, if any?  When the angle is 0, the
% incidence plane is not defined.  Therefore it is not possible to be parallel
% to it.  Then again, it is not really possible to be perpendicular to it
% either.
%
% Hecht is mentioning something in his section about the Fresnel equations: the
% choice of reference frame matters.  By choosing another reference frame for
% the perp case, we can flip the sign of $r_perp$ and leave the rest untouched.
% Actually, when I look at his figures 4.39 and 4.40, I am not surprised that
% his signs are messed up since his vectors do not overlap in the same way when
% the angles converge toward 0.  Indeed, Ei and Et overlap in his perp case,
% but not in his para case.  So if I fix this, I get a minus sign for
% $r_para$.

\begin{figure}[hbtp]
    \centering
    \input{figures/fresnel_reference_frames.eps_tex}
    \caption{\label{fig:fresnel_directions}Field directions chosen to express the Fresnel equations.}
    \caption*{
        $n$ is normal to the interface, $u$ is normal to the plane of incidence.
        The vectors~$p_i$ indicate the direction of the field parallel to the plane-of-incidence (plane of the page) for each port.  The vectors~$s_i$ do the same for the perpendicular direction.
    }
\end{figure}

Before we can apply the reflection and transmission coefficients given in~\eqref{eq:fresnel_oblique}, we need to decompose the incident field into a parallel and a perpendicular component.
The incident field can be decomposed into a component that is parallel to the plane-of-incidence and one that is perpendicular to it.

We need notations.
I call $k$ the direction of propagation, $n$ the normal to the interface, and $u$ the normal to the plane-of-incidence (because $p$ and $i$ are already taken).

We get $e_\perp$ by projecting $e$ on the unit vector $u$ according to equation~\eqref{eq:paraperp_1}.
\begin{equation}
    e_\perp = u \times (u \cdot e) \label{eq:paraperp_1}
\end{equation}
In equation~\eqref{eq:paraperp_1}, $u \cdot e$~represents the inner product (dot product) of~$u$ by~$e$ and the symbol~``$\times$'' refers to the multiplication of a matrix (here~$u$) by a scalar (here~$u \cdot e$).
We wish to replace that scalar multiplication with a matrix multiplication.
To do that, we replace the scalar~$u \cdot e$ by the 1--by--1 matrix~$[u \cdot e]$ containing that scalar as shown in equation~\eqref{eq:paraperp_2}.
We do not use any symbol for the matrix multiplication, we simply juxtapose the matrices.
\begin{equation}
    e_\perp = u [u \cdot e] \label{eq:paraperp_2}
\end{equation}
The dimensions match: $u$~is a 3--by--1 matrix and $[u \cdot e]$~is a 1--by--1 matrix, resulting in~$e_\perp$ being a 3--by--1 matrix.
For the next step, we note that the inner product $u \cdot e$ is related to the outer product $u\transp e$ by equation~\eqref{eq:paraperp_3}.
\begin{equation}
    [u \cdot e] = u\transp e \label{eq:paraperp_3}
\end{equation}
Injecting equation~\eqref{eq:paraperp_3} into equation~\eqref{eq:paraperp_2} gives equation~\eqref{eq:paraperp_4}.
\begin{equation}
    e_\perp = u \left( u\transp e \right) \label{eq:paraperp_4}
\end{equation}
By associativity of the matrix multiplication we can transform equation~\eqref{eq:paraperp_4} into equation~\eqref{eq:para_perp_decomposition_field}.
Then, by identification, we can find our decomposition matrices.
In equation~\eqref{eq:para_perp_decomposition_matrix}, the matrices~$M_\perp$ and~$M_\parallel$ decompose a vector~$e$ into its perpendicular and parallel components~$e_\perp$ and~$e_\parallel$ on which we can apply the Fresnel equations~\eqref{eq:fresnel_oblique}.
\begin{gather}
    \begin{aligned}
        e_\perp &= \left(u u\transp \right) e
        \\
        e_\parallel &= e - e_\perp
    \end{aligned}
    \label{eq:para_perp_decomposition_field}
    \\
    \begin{aligned}
        M_\perp &= u u\transp   \\
        M_\parallel & = I - M_\perp
    \end{aligned}
    \label{eq:para_perp_decomposition_matrix}
\end{gather}

One more step is required: we need to compute $u$ from $n$ and $k$.
This step is very straightforward.
Since $u$ is normal to the plane-of-incidence, and since the plane-of-incidence contains $n$ and $k$, then $u$ is normal to both $n$ and $k$.
In other words, $u$ is collinear to the cross product of $n$ and $k$.
We know that the cross-product is antisymetric but in our case the order does not matter; indeed, the sign cancels itself out in $u u^T$.
\begin{equation}
    u = \frac{1}{\norm{n \times k}} n \times k
\end{equation}
The order will matter in the next part when we consider the geometry of the system.

We are now able to calculate the reflected and transmitted fields of an incident plane wave on an interface at oblique incidence.
However, the results are in a reference frame local to the reflected or transmitted wave, not a global reference frame shared by the three waves.
Figure~\ref{fig:fresnel_rotations} shows how the reflected and transmitted reference frames are obtained by rotating the incident reference frame around~$u$, the normal to the plane-of-incidence.
\begin{figure}[hbtp]
    \centering
    \missingfigure{Fresnel rotations.}
    \caption{\label{fig:fresnel_rotations}Fresnel rotations.}
\end{figure}

The angle-of-incidence, noted $\theta_i$, is defined in equation~\eqref{eq:angle_of_incidence_definition} as the angle between the incident direction of propagation~$k_i$ and the normal to the surface~$n$.
\begin{equation}
    \theta_i \equiv \widehat{n k_i} \pmod{2\pi}
    \label{eq:angle_of_incidence_definition}
\end{equation}
Equation~\eqref{eq:dot_product_cos} shows a relation between the dot-product of two vectors and the angle between them.
\begin{equation}
    v \cdot w = \norm{v} \norm{w} \cos \widehat{vw}
    \label{eq:dot_product_cos}
\end{equation}
The dot-product~$v \cdot w$ can also be calculated by summing the component-wise product of $v$ and $w$ (equation~\eqref{eq:dot_product_elementwise}, which is a method that does not require knowing~$\widehat{vw}$.
\begin{equation}
    v \cdot w = \sum_i v_i w_i
    \label{eq:dot_product_elementwise}
\end{equation}
Equations~\eqref{eq:dot_product_cos} and~\eqref{eq:dot_product_elementwise} form a system from which we can extract the angle~$\widehat{uv}$.
Its solution is given in equation~\eqref{eq:angle_from_dot_product}, provided that the angle is between 0 and $\pi$.
This is due to a limitation of the range of the $\arccos$ function.
\begin{equation}
    \left\lbrace
        \begin{aligned}
            \widehat{vw} &\in [0, \pi]
            \\
            \widehat{vw} &= \arccos
            \left(
                \frac{
                    \sum_i v_i w_i
                }{
                    \norm{v} \norm{w}
                }
            \right)
        \end{aligned}
    \right.
    \label{eq:angle_from_dot_product}
\end{equation}
The limitation~$\widehat{vw} \in [0, \pi]$ does not impede us.
Indeed, $\theta_i \in ]0, \pi/2[$.
\begin{itemize}
    \item 0 is excluded because this corresponds to the case of normal incidence which results in a two-ports network and not a four-ports network.  We have treated this particular case in section~\vref{sec:interface_at_normal_incidence}.
    \item $\pi/2$ is excluded because this corresponds to a direction of propagation parallel to the surface, the wave would therefore never hit the interface.
    \item Angles between $\pi/2$ and $\pi$ are excluded because the incident wave would be coming from the wrong side of the interface.
    \item Angles between 0 and $-\pi$ are excluded because the rotation axis~$u$ adapts its direction to make sure that the angle from $n$ to $k_i$ is always positive from the point of view of~$u$.
    \item The other angles fall into the previous categories because of the modulo~$2\pi$.
\end{itemize}
It is therefore safe to apply equation~\eqref{eq:angle_from_dot_product} to retreive~$\theta_i$ from $n$ and $k_i$.

The vectors~$n$ and $k_1$ and the refractive incides $n_a$ and $n_b$ are sufficient to determine the geometry of the whole network.
In other words, the orientation of one port constrains that of the three other ports.
Because each port can be seen as an incident, reflected or transmitted port, I shall stop using the notation $\theta_i$, $\theta_r$ and $\theta_t$;
instead, I shall use $\theta_a$ and $\theta_b$.
$\theta_a$ refers to the angle between the direction of propagation and the normal to the interface on the A side of the interface;
likewise, $\theta_b$ refers to the corresponding angle on the B side.
\index{Snell's law}The angles~$\theta_a$ and~$\theta_b$ are related to the refractive incides~$n_a$ and~$n_b$ of the propagation media A and B by Snell's Law~\eqref{eq:snell}.
\begin{gather}
    \Re(n_a) \sin \theta_a = \Re(n_b) \sin \theta_b
    \label{eq:snell}
    \\
    \left\lbrace
        \begin{aligned}
            \theta_b &\in [-\pi/2, \pi/2]
            \\
            \theta_b &= \arcsin
            \left(
                \frac{\Re(n_a)}{\Re(n_b)}
                \sin \theta_a
            \right)
        \end{aligned}
    \right.
    \label{eq:snell_thetab}
\end{gather}
Once again, the limited range of~$\theta_b$ does not impede us.

Upon reflection and refraction, the direction of the electric field rotates around $u$.
Since there is no connection between the port 1 and the port 4, and between the port 2 and the port 3, we are interested in eight angles only: $\theta_{1 \rightarrow 2}$, $\theta_{1 \rightarrow 3}$, $\theta_{2 \rightarrow 1}$, $\theta_{2 \rightarrow 4}$, $\theta_{3 \rightarrow 1}$, $\theta_{3 \rightarrow 4}$, $\theta_{4 \rightarrow 2}$ and $\theta_{4 \rightarrow 3}$.
The values for these angles are listed in equations~\eqref{eq:interface_rotations}.
\begin{equation}
    \begin{aligned}
        \theta_{1 \rightarrow 2} &= \pi - 2\theta_a
        &
        \theta_{2 \rightarrow 1} &= -\pi + 2\theta_a
        \\
        \theta_{3 \rightarrow 4} &= \pi - 2\theta_b
        &
        \theta_{4 \rightarrow 3} &= -\pi + 2\theta_b
        \\
        \theta_{1 \rightarrow 3} &= \theta_b - \theta_a
        &
        \theta_{3 \rightarrow 1} &= \theta_a - \theta_b
        \\
        \theta_{2 \rightarrow 4} &= \theta_a - \theta_b
        &
        \theta_{4 \rightarrow 2} &= \theta_b - \theta_a
    \end{aligned}
    \label{eq:interface_rotations}
\end{equation}
To each angle $\theta$ of equation~\eqref{eq:interface_rotations} corresponds a rotation matrix $R$.
From an implementation point of view, computing the matrix corresponding to a rotation around an axis (here $u$) may be best done using a quaternion.
We do not need to enter into details, the two following steps are all we need to create the rotation matrix we want.
First, we construct the quaternion corresponding to a rotation of~$\theta$ around a vector $u=(u_x, u_y, y_z)$ with equation~\eqref{eq:quaternion_rotation_around_axis}.
\begin{equation}
    \begin{aligned}
        q &= w + xi + yj + zk
        \\
        q &= \cos \frac{\theta}{2}
           + u_x \sin \frac{\theta}{2} i
           + u_y \sin \frac{\theta}{2} j
           + u_z \sin \frac{\theta}{2} k
    \end{aligned}
    \label{eq:quaternion_rotation_around_axis}
\end{equation}
Then we convert that quaternion into a rotation matrix with equation~\eqref{eq:quaternion_to_rotation_matrix}.
\begin{equation}
    R =
    \begin{pmatrix}
        1 - 2y^2 - 2z^2   &   2xy - 2zw         &   2xz + 2yw \\
        2xy + 2zw         &   1 - 2x^2 - 2z^2   &   2yz - 2xw \\
        2xz - 2yw         &   2yz + 2xw         &   1 - 2x^2 - 2y^2
    \end{pmatrix}
    \label{eq:quaternion_to_rotation_matrix}
\end{equation}

We can finally put it all together.
Equation~\eqref{eq:interface_S} lists all the elements of the scattering matrix of an interface at normal indicence.
The matrices $R$ are rotation matrices produced by~\eqref{eq:quaternion_to_rotation_matrix} for each angle~\eqref{eq:interface_rotations}.
The matrices $M_\parallel$ and $M_\perp$ are defined in~\eqref{eq:para_perp_decomposition_matrix} and decompose the fields into their parallel and perpendicular components.
The various~$r$ and~$t$ refer to the Fresnel equations~\eqref{eq:fresnel_oblique}; the subscript~$a$ denoting incidence on the A side of the interface, and~$b$ to the B side of the interface.
\begin{equation}
    \begin{gathered}
    \begin{aligned}
        S_{1, 2} &= R^{2 \rightarrow 1} \left(
            r_{\parallel a} M_\parallel +
            r_{\perp a} M_\perp
        \right)
        &
        S_{2, 1} &= R^{1 \rightarrow 2} \left(
            r_{\parallel a} M_\parallel +
            r_{\perp a} M_\perp
        \right)
        \\
        S_{1, 3} &= R^{3 \rightarrow 1} \left(
            t_{\parallel b} M_\parallel +
            t_{\perp b} M_\perp
        \right)
        &
        S_{3, 1} &= R^{1 \rightarrow 3} \left(
            t_{\parallel a} M_\parallel +
            t_{\perp a} M_\perp
        \right)
        \\
        S_{2, 4} &= R^{4 \rightarrow 2} \left(
            t_{\parallel b} M_\parallel +
            t_{\perp b} M_\perp
        \right)
        &
        S_{4, 2} &= R^{2 \rightarrow 4} \left(
            t_{\parallel a} M_\parallel +
            t_{\perp a} M_\perp
        \right)
        \\
        S_{3, 4} &= R^{4 \rightarrow 3} \left(
            r_{\parallel b} M_\parallel +
            r_{\perp b} M_\perp
        \right)
        &
        S_{4, 3} &= R^{3 \rightarrow 4} \left(
            r_{\parallel b} M_\parallel +
            r_{\perp b} M_\perp
        \right)
        \\
    \end{aligned}
    \\
    S_{1, 1} = S_{1, 4} = S_{2, 2} = S_{2, 3} = S_{3, 2} = S_{3, 3} = S_{4, 1} = S_{4, 4} = 0
    \end{gathered}
    \label{eq:interface_S}
\end{equation}
This concludes our work on the scattering matrix of the interface between two linear homogeneous isotropic media.
However, for this network to find its place in a system, we also need to compute the directions of propagations $k_2$, $k_3$ and $k_4$: these may be required by other networks.

\begin{figure}[hbtp]
    \centering
    \missingfigure{fig:interface propagation rotation}
    \caption{\label{fig:interface_propagation_rotation}interface propagation rotation}
\end{figure}
Equations~\eqref{eq:interface_propagation_rotation} lists the rotations matrices that transform~$k_1$ into $k_2$, $k_3$ and $k_4$, represented in figure~\ref{fig:interface_propagation_rotation}.
\begin{equation}
    \begin{aligned}
        k_2 &= R^{1 \rightarrow 2} k_1 \\
        k_3 &= R^{1 \rightarrow 3} k_1 \\
        k_4 &= R^{1 \rightarrow 4} k_1 \\
    \end{aligned}
    \label{eq:interface_propagation_rotation}
\end{equation}
We already know~$R^{1 \rightarrow 2}$ and~$R^{1 \rightarrow 3}$ (see equations~\eqref{eq:interface_rotations}) but we do not have~$R^{1 \rightarrow 4}$ yet.
The angle~$\theta_{1 \rightarrow 4}$ is easy to determine with figure~\ref{fig:interface_propagation_rotation} and is given by equation~\eqref{eq:interface_rotation_1_to_4}.
We can use~\eqref{eq:quaternion_rotation_around_axis} and~\eqref{eq:quaternion_to_rotation_matrix} to derive~$R^{1 \rightarrow 4}$ from~$\theta_{1 \rightarrow 4}$.
\begin{equation}
    \theta_{1 \rightarrow 4} = -\theta_a - \theta_b
    \label{eq:interface_rotation_1_to_4}
\end{equation}
With that final step, we can now connect the network to other networks since we can provide them with their own~$k_1$.



%=============================================================================

\subsection{Wire grid polarizer}
In their paper from \citeyear{houde_2001} \citetitle{houde_2001}, \textcite{houde_2001} present a set of equations that approximate the electric field at any point near a wire grid polarizer.
The incident wave is assumed plane, the surrounding propagation medium is assumed homogeneous and isotropic, and the wires are assumed to be free floating (no dielectric substrate) and to have a cylindrical section.

The set of equations that interest us here are the equations numbered 23 to 35, 62 and 63 in \cite{houde_2001}.
\begin{align}
    K^x &= \frac{E_0}{F} \cdot \alpha' \frac{N_x}{\Delta_x}
    \\
    K^\theta &= (-j) \frac{E_0}{F} \cdot (\gamma' \beta - \beta' \gamma) \frac{N_\theta}{\Delta_\theta}
\end{align}
with
\begin{align}
    N_x
    &=
    1 - j \frac{Z_s}{Z_0} \frac{ka}{2}
    \\
    \Delta_x
    &=
    (1 - \alpha^2) S_1 - j \frac{Z_s}{Z_0} \sqrt{1 - \alpha^2}H_1^{(2)} (k'a)
    \\
    N_\theta
    &=
    1 + j \frac{Z_s}{Z_0} \frac{2}{ka}
    \\
    \Delta_\theta
    &=
    \sqrt{1 - \alpha^2} H_1^{(2)} (k'a) + j \frac{Z_s}{Z_0} (1 - \alpha^2) S_1
\end{align}
and
\begin{equation}
    S_1 = H_0^{(2)} (k'a) + 2
    \sum_{n=1}^\infty
    H_0^{(2)}(k'nd) \cos (k \beta nd)
    \text{.}
    \label{eq:infinite_hankel}
\end{equation}
The following equations use the previous definitions.
They describe the reflected and transmitted fields in three dimensions for a grid in the $xy$-plane, wires along~$x$.
\begin{align}
    R^x
    &=
    -\frac{F}{E_0}
    \frac{\lambda}{\pi d}
    \frac{1 - \alpha^2}{\gamma} K^x
    \label{eq:houde_Rx}
    \\
    R^y
    &=
    \phantom{-}
    \frac{F}{E_0}
    \frac{\lambda}{\pi d}
    \left[
        \frac{\alpha \beta}{\gamma} K^x
        -
        j \frac{ka}{2} K^\theta
    \right]
    \\
    R^z
    &=
    -\frac{F}{E_0}
    \frac{\lambda}{\pi d}
    \left[
       \alpha K^x
       +
       j \frac{\beta}{\gamma} \frac{ka}{2} K^\theta
    \right]
    \\
    T^x &= \alpha' + R^x
    \\
    T^y
    &=
    \beta' +
    \frac{F}{E_0}
    \frac{\lambda}{\pi d}
    \left[
        \frac{\alpha \beta}{\gamma} K^x + j \frac{ka}{2} K^\theta
    \right]
    \\
    T^z
    &=
    \gamma' +
    \frac{F}{E_0}
    \frac{\lambda}{\pi d}
    \left[
        \alpha K^x - j \frac{\beta}{\gamma} \frac{ka}{2} K^\theta
    \right]
\end{align}
Note that Houde calls these $R$ and $T$ ``reflection'' and ``transmission coefficient'', something they are not quite that since they contain $\alpha'$, $\beta'$ and $\gamma'$, corresponding to the amplitude of the incoming electric field.
These amplitudes have to be factored out if we want to really speak about reflection or transmission coefficients.

When implementing these equations, some simplifications are obvious.
For example, $\frac{E_0}{F}$ inside $K^x$ and $K^\theta$ cancels $\frac{F}{E_0}$ in $R$ and $T$;
the complex unit $j$ within $k^\theta$ combines with $j$ in $R$ and $T$ to become -1.

In these equations, $\alpha'$, $\beta'$ and $\gamma'$ are the three components of the direction of the incident electric field, satisfying $\alpha'^2 + \beta'^2 + \gamma'^2 = 1$.
The incident electric field is $(e_x, e_y, e_z) = E_0(\alpha', \beta', \gamma')$.
In order to rewrite these equations in a matrix form that depends on $e_x$, $e_y$ and $e_z$, one needs to split each coefficient of transmission and reflection into three, like this:
\begin{equation}
    e_{rx} = R^{xx} e_x + R^{xy} e_y + R^{xz} e_z
\end{equation}
so that we can write
\begin{equation}
    e_r = R 
    \begin{pmatrix}
        R_{xx} & R_{xy} & R_{xz} \\
        R_{yx} & R_{yy} & R_{yz} \\
        R_{zx} & R_{zy} & R_{zz}
    \end{pmatrix}
    e_i
\end{equation}
for the reflection $R$ and a similar equation for the transmission $e_t = T e_i$.
I start with $R_x$ defined in equation~\eqref{eq:houde_Rx} to get $R_{xx}$, $R_{xy}$ and $R_{xz}$.
\begin{align*}
    e_{rx} &= R^x E_0
    \\
           &= -\frac{F}{E_0}
              \frac{\lambda}{\pi d}
              \frac{1-\alpha^2}{\gamma}
              K^x
              E_0
    \\
           &= -\cancel{\frac{F}{E_0}}
              \frac{\lambda}{\pi d}
              \frac{1-\alpha^2}{\gamma}
              \cancel{\frac{E_0}{F}}
              \frac{N_x}{\Delta_x}
              \underbrace{
                  \alpha'
                  E_0
              }_{e_{ix}}
    \\
           &= -\frac{\lambda}{\pi d}
              \frac{1-\alpha^2}{\gamma}
              \frac{N_x}{\Delta_x}
              e_{ix}
\end{align*}
By identification, we find these values for $R_{xx}$, $R_{xy}$ and $R_{xz}$.
\begin{equation}
    \left\lbrace
    \begin{aligned}
        R_{xx} &= -\frac{\lambda}{\pi d}
                  \frac{1-\alpha^2}{\gamma}
                  \frac{N_x}{\Delta_x}
        \\
        R_{xy} &= 0
        \\
        R_{xz} &= 0
    \end{aligned}
    \right.
\end{equation}
Let us continue with $R_y$.
\begin{align*}
    e_{ry}
    &= R^y E_0
    \\
    &= \frac{F}{E_0}
       \frac{\lambda}{\pi d}
       \left[
           \frac{\alpha \beta}{\gamma}
           K^x
           -
           j
           \frac{\beta}{\alpha}
           \frac{ka}{2}
           K^\theta           
       \right]
       E_0
    \\
    &= \cancel{\frac{F}{E_0}}
       \frac{\lambda}{\pi d}
       \left[
           \frac{\alpha \beta}{\gamma}
           \cancel{\frac{E_0}{F}}
           \frac{N_x}{\Delta_x}
           \alpha'
           -
           j
           \frac{\beta}{\alpha}
           \frac{ka}{2}
           (-j)
           \cancel{\frac{E_0}{F}}
           \frac{N_\theta}{\Delta_\theta}
           (\gamma' \beta - \beta' \gamma)           
       \right]
       E_0
    \\
    &= \frac{\lambda}{\pi d}
       \left[
           \frac{\alpha \beta}{\gamma}
           \frac{N_x}{\Delta_x}
           \alpha'
           +
           \frac{\beta}{\alpha}
           \frac{ka}{2}
           \frac{N_\theta}{\Delta_\theta}
           \gamma
           \beta'
           -
           \frac{\beta}{\alpha}
           \frac{ka}{2}
           \frac{N_\theta}{\Delta_\theta}
           \beta
           \gamma'
       \right]
       E_0
    \\
    &= \frac{\lambda}{\pi d}
       \frac{\alpha \beta}{\gamma}
       \frac{N_x}{\Delta_x}
       \underbrace{E_0 \alpha'}_{e_{ix}}
       +
       \frac{\lambda}{\pi d}
       \frac{\beta}{\alpha}
       \frac{ka}{2}
       \frac{N_\theta}{\Delta_\theta}
       \gamma
       \underbrace{E_0 \beta'}_{e_{iy}}
       -
       \frac{\lambda}{\pi d}
       \frac{\beta}{\alpha}
       \frac{ka}{2}
       \frac{N_\theta}{\Delta_\theta}
       \beta
       \underbrace{E_0 \gamma'}_{e_{iz}}
\end{align*}
\begin{equation}
    \left\lbrace
    \begin{aligned}
        R_{yx}
        &=
        \phantom{-}
        \frac{\lambda}{\pi d}
        \frac{\alpha \beta}{\gamma}
        \frac{N_x}{\Delta_x}
        \\
        R_{yy}
        &=
        \phantom{-}
        \frac{\lambda}{\pi d}
        \frac{\beta}{\alpha}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \gamma
        \\
        R_{yz}
        &=
        -
        \frac{\lambda}{\pi d}
        \frac{\beta}{\alpha}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \beta
    \end{aligned}
    \right.
\end{equation}
Same thing for $R_z$.
\begin{align*}
    e_{rz} &= R^z E_0
    \\
    &=
    -
    \frac{F}{E_0}
    \frac{\lambda}{\pi d}
    \left[
        \alpha K^x
        +
        j
        \frac{\beta}{\gamma}
        \frac{ka}{2}
        k^\theta
    \right]
    E_0
    \\
    &=
    -
    \cancel{\frac{F}{E_0}}
    \frac{\lambda}{\pi d}
    \left[
        \alpha
        \cancel{\frac{E_0}{F}}
        \frac{N_x}{\Delta_x}
        \alpha'
        +
        j
        \frac{\beta}{\gamma}
        \frac{ka}{2}
        (-j)
        \cancel{\frac{E_0}{F}}
        (\gamma' \beta - \beta' \gamma)
        \frac{N_\theta}{\Delta_\theta}
    \right]
    E_0
    \\
    &=
    -
    \frac{\lambda}{\pi d}
    \alpha
    \frac{N_x}{\Delta_x}
    \underbrace{E_0 \alpha'}_{e_{ix}}
    +
    \frac{\lambda}{\pi d}
    \frac{\beta}{\gamma}
    \frac{ka}{2}
    \frac{N_\theta}{\Delta_\theta}
    \gamma
    \underbrace{E_0 \beta'}_{e_{iy}}
    -
    \frac{\lambda}{\pi d}
    \frac{\beta}{\gamma}
    \frac{ka}{2}
    \frac{N_\theta}{\Delta_\theta}
    \beta
    \underbrace{E_0 \gamma'}_{e_{iz}}
\end{align*}
\begin{equation}
    \left\lbrace
    \begin{aligned}
        R_{zx}
        &=
        -
        \frac{\lambda}{\pi d}
        \alpha
        \frac{N_x}{\Delta_x}
        \\
        R_{zy}
        &=
        \phantom{-}
        \frac{\lambda}{\pi d}
        \frac{\beta}{\gamma}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \gamma
        \\
        R_{zz}
        &=
        -
        \frac{\lambda}{\pi d}
        \frac{\beta}{\gamma}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \beta
    \end{aligned}
    \right.
\end{equation}
We have the reflection matrix.
Now, we compute the transmission matrix.
\begin{align*}
    e_{tx} &= T^x E_0
    \\
    &= (\alpha' + R^x) E_0
    \\
    &= \underbrace{\alpha' E_0}_{e_{ix}}
       -
       \frac{\lambda}{\pi d}
       \frac{1 - \alpha^2}{\gamma}
       \frac{N_x}{\Delta_x}
       \underbrace{\alpha' E_0}_{e_{ix}}
    \\
    &= \left(
           1
           -
           \frac{\lambda}{\pi d}
           \frac{1 - \alpha^2}{\gamma}
           \frac{N_x}{\Delta_x}
       \right)
       e_{ix}
\end{align*}
\begin{equation}
    \left\lbrace
    \begin{aligned}
        T_{xx}
        &= 1
           -
           \frac{\lambda}{\pi d}
           \frac{1 - \alpha^2}{\gamma}
           \frac{N_x}{\Delta_x}
        \\
        T_{xy} &= 0
        \\
        T_{xz} &= 0
    \end{aligned}
    \right.
\end{equation}

\begin{align*}
    e_{ty} &= T^y E_0
    \\
    &=
    \left(
        \beta'
        +
        \frac{F}{E_0}
        \frac{\lambda}{\pi d}
        \left[
            \frac{\alpha \beta}{\gamma}
            K^x
            +
            j
            \frac{ka}{2}
            K^\theta
        \right]
    \right)
    E_0
    \\
    &=
    \left(
        \beta'
        +
        \cancel{\frac{F}{E_0}}
        \frac{\lambda}{\pi d}
        \left[
            \frac{\alpha \beta}{\gamma}
            \cancel{\frac{E_0}{F}}
            \frac{N_x}{\Delta_x}
            \alpha'
            +
            j
            \frac{ka}{2}
            (-j)
            \cancel{\frac{E_0}{F}}
            \frac{N_\theta}{\Delta_\theta}
            (\gamma' \beta - \beta' \gamma)
        \right]
    \right)
    E_0
    \\
    &=
    \left(
        \beta'
        +
        \frac{\lambda}{\pi d}
        \frac{\alpha \beta}{\gamma}
        \frac{N_x}{\Delta_x}
        \alpha'
        -
        \frac{\lambda}{\pi d}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \gamma
        \beta'
        +
        \frac{\lambda}{\pi d}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \beta
        \gamma'
    \right)
    E_0
    \\
    &=
    \frac{\lambda}{\pi d}
    \frac{\alpha \beta}{\gamma}
    \frac{N_x}{\Delta_x}
    \underbrace{E_0 \alpha'}_{e_{ix}}
    +
    \left(
        1
        -
        \frac{\lambda}{\pi d}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \gamma
    \right)
    \underbrace{E_0 \beta'}_{e_{iy}}
    +
    \frac{\lambda}{\pi d}
    \frac{ka}{2}
    \frac{N_\theta}{\Delta_\theta}
    \beta
    \underbrace{E_0 \gamma'}_{e_{iz}}
\end{align*}
\begin{equation}
    \left\lbrace
    \begin{aligned}
        T_{yx}
        &= \frac{\lambda}{\pi d}
           \frac{\alpha \beta}{\gamma}
           \frac{N_x}{\Delta_x}
        \\
        T_{yy}
        &= 1
           -
           \frac{\lambda}{\pi d}
           \frac{ka}{2}
           \frac{N_\theta}{\Delta_\theta}
           \gamma
        \\
        T_{yz}
        &= \frac{\lambda}{\pi d}
           \frac{ka}{2}
           \frac{N_\theta}{\Delta_\theta}
           \beta
    \end{aligned}
    \right.
\end{equation}

\begin{align*}
    e_{tz} &= T^z E_0
    \\
    &=
    \left(
        \gamma' +
        \frac{F}{E_0}
        \frac{\lambda}{\pi d}
        \left[
            \alpha K^x - j \frac{\beta}{\gamma} \frac{ka}{2} K^\theta
        \right]
    \right)
    E_0
    \\
    &=
    \left(
        \gamma' +
        \cancel{\frac{F}{E_0}}
        \frac{\lambda}{\pi d}
        \left[
            \alpha
            \cancel{\frac{E_0}{F_0}}
            \frac{N_x}{\Delta_x}
            \alpha'
            -
            j
            \frac{\beta}{\gamma}
            \frac{ka}{2}
            (-j)
            \cancel{\frac{E_0}{F}}
            \frac{N_\theta}{\Delta_\theta}
            (\gamma' \beta - \beta' \gamma)
        \right]
    \right)
    E_0
    \\
    &=
    \left(
        \gamma' +
        \frac{\lambda}{\pi d}
        \left[
            \alpha
            \frac{N_x}{\Delta_x}
            \alpha'
            +
            \frac{\beta}{\gamma}
            \frac{ka}{2}
            \frac{N_\theta}{\Delta_\theta}
            \gamma
            \beta'
            -
            \frac{\beta}{\gamma}
            \frac{ka}{2}
            \frac{N_\theta}{\Delta_\theta}
            \beta
            \gamma'
        \right]
    \right)
    E_0
    \\
    &=
    \frac{\lambda}{\pi d}
    \alpha
    \frac{N_x}{\Delta_x}
    \underbrace{E_0 \alpha'}_{e_{ix}}
    +
    \frac{\lambda}{\pi d}
    \frac{\beta}{\gamma}
    \frac{ka}{2}
    \frac{N_\theta}{\Delta_\theta}
    \gamma
    \underbrace{E_0 \beta'}_{e_{iy}}
    +
    \left(
        1
        -
        \frac{\lambda}{\pi d}
        \frac{\beta}{\gamma}
        \frac{ka}{2}
        \frac{N_\theta}{\Delta_\theta}
        \beta
    \right)
    \underbrace{E_0 \gamma'}_{e_{iz}}
\end{align*}
\begin{equation}
    \left\lbrace
    \begin{aligned}
        T_{zx}
        &= \frac{\lambda}{\pi d}
           \alpha
           \frac{N_x}{\Delta_x}
        \\
        T_{zy}
        &= \frac{\lambda}{\pi d}
           \frac{\beta}{\gamma}
           \frac{ka}{2}
           \frac{N_\theta}{\Delta_\theta}
           \gamma
        \\
        T_{zz}
        &= 1
           -
           \frac{\lambda}{\pi d}
           \frac{\beta}{\gamma}
           \frac{ka}{2}
           \frac{N_\theta}{\Delta_\theta}
           \beta
    \end{aligned}
    \right.
\end{equation}

%#############################################################################

\section{Simple systems}

The idea is to show that it works and makes sense.

I must illustrate the most simple cavity.  I show it creates a standing wave pattern, I show its FFT, prove it's consistent with the length of the cavity and its refractive index.

Then I introduce a thin film beam splitter (that's already a heterodyne telescope when we think of it).  I show that it adds a slope to the previous pattern.  I show that it's actually a very slow standing wave, the period of which is consistent with infinite reflections inside the thin film.  The FFT should show that there are two picks close to the cavity-period: they are due to the fact that there are now two cavities: one using reflection on the near side, one on the far side of the beam splitter.

All of this makes sense, we are confident that the model does its job properly.
